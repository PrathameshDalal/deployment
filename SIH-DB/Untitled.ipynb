{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a6815c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"vivit is not supported yet. Only ['albert', 'bart', 'beit', 'bert', 'big-bird', 'bigbird-pegasus', 'blenderbot', 'blenderbot-small', 'bloom', 'camembert', 'clip', 'codegen', 'convbert', 'convnext', 'data2vec-text', 'data2vec-vision', 'deberta', 'deberta-v2', 'deit', 'detr', 'distilbert', 'electra', 'flaubert', 'gpt2', 'gptj', 'gpt-neo', 'groupvit', 'ibert', 'imagegpt', 'layoutlm', 'layoutlmv3', 'levit', 'longt5', 'longformer', 'marian', 'mbart', 'mobilebert', 'mobilenet-v1', 'mobilenet-v2', 'mobilevit', 'mt5', 'm2m-100', 'owlvit', 'perceiver', 'poolformer', 'rembert', 'resnet', 'roberta', 'roformer', 'segformer', 'squeezebert', 'swin', 't5', 'vision-encoder-decoder', 'vit', 'whisper', 'xlm', 'xlm-roberta', 'yolos'] are supported. If you want to support vivit please propose a PR or open up an issue.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForVideoClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# load config\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m model_kind, model_onnx_config \u001b[38;5;241m=\u001b[39m \u001b[43mFeaturesManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_supported_model_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m onnx_config \u001b[38;5;241m=\u001b[39m model_onnx_config(model\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# export\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\onnx\\features.py:728\u001b[0m, in \u001b[0;36mFeaturesManager.check_supported_model_or_raise\u001b[1;34m(model, feature)\u001b[0m\n\u001b[0;32m    726\u001b[0m model_type \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    727\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 728\u001b[0m model_features \u001b[38;5;241m=\u001b[39m \u001b[43mFeaturesManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_supported_features_for_model_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_features:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    731\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt support feature \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Supported values are: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    732\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\onnx\\features.py:575\u001b[0m, in \u001b[0;36mFeaturesManager.get_supported_features_for_model_type\u001b[1;34m(model_type, model_name)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m FeaturesManager\u001b[38;5;241m.\u001b[39m_SUPPORTED_MODEL_TYPE:\n\u001b[0;32m    574\u001b[0m     model_type_and_model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;28;01melse\u001b[39;00m model_type\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type_and_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not supported yet. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(FeaturesManager\u001b[38;5;241m.\u001b[39m_SUPPORTED_MODEL_TYPE\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want to support \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m please propose a PR or open up an issue.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    579\u001b[0m     )\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m FeaturesManager\u001b[38;5;241m.\u001b[39m_SUPPORTED_MODEL_TYPE[model_type]\n",
      "\u001b[1;31mKeyError\u001b[0m: \"vivit is not supported yet. Only ['albert', 'bart', 'beit', 'bert', 'big-bird', 'bigbird-pegasus', 'blenderbot', 'blenderbot-small', 'bloom', 'camembert', 'clip', 'codegen', 'convbert', 'convnext', 'data2vec-text', 'data2vec-vision', 'deberta', 'deberta-v2', 'deit', 'detr', 'distilbert', 'electra', 'flaubert', 'gpt2', 'gptj', 'gpt-neo', 'groupvit', 'ibert', 'imagegpt', 'layoutlm', 'layoutlmv3', 'levit', 'longt5', 'longformer', 'marian', 'mbart', 'mobilebert', 'mobilenet-v1', 'mobilenet-v2', 'mobilevit', 'mt5', 'm2m-100', 'owlvit', 'perceiver', 'poolformer', 'rembert', 'resnet', 'roberta', 'roformer', 'segformer', 'squeezebert', 'swin', 't5', 'vision-encoder-decoder', 'vit', 'whisper', 'xlm', 'xlm-roberta', 'yolos'] are supported. If you want to support vivit please propose a PR or open up an issue.\""
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import transformers\n",
    "from transformers.onnx import FeaturesManager\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import VivitImageProcessor, VivitForVideoClassification\n",
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification \n",
    "from transformers import AutoTokenizer, AutoModelForVideoClassification\n",
    "from transformers import pipeline\n",
    "# load model and tokenizer\n",
    "model_id = \"prathameshdalal/vivit-b-16x2-kinetics400-UCF-Crime\"\n",
    "feature = \"video-classification\"\n",
    "model = AutoModelForVideoClassification.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# load config\n",
    "model_kind, model_onnx_config = FeaturesManager.check_supported_model_or_raise(model, feature=feature)\n",
    "onnx_config = model_onnx_config(model.config)\n",
    "\n",
    "# export\n",
    "onnx_inputs, onnx_outputs = transformers.onnx.export(\n",
    "        preprocessor=VivitImageProcessor.from_pretrained(\"google/vivit-b-16x2-kinetics400\"),\n",
    "        model=model,\n",
    "        config=onnx_config,\n",
    "        opset=13,\n",
    "        output=Path(\"trfs-model.onnx\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ebb9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
